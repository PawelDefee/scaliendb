Sequential storage which is never split: TODO: xxx
--------------------------------------------------

Memorychunks
------------

Each shard has one or many in-memory chunks. These are either pinned (not yet written) or unpinned (read from disk, may be purged from memory).

When a write operation issued, it is put in the redo log. The redo log itself is not kept in memory.

Then it is put into the shard's active in-memory chunk. Here KV/buffers are allocated, and marked as pinned into memory.

Delete commands create special tombstone entries in the tree.

The redo log is closed and a new one re-opened when it grows beyond 128MB. Call this log-cycling.

When the redo log is cycled, the in-memory chunks are frozen in-memory. As a new redo log is opened, new in-memory chunks are created.

Eventually, these in-memory chunks are written to disk as a sorted file with an index page and bloom filter-like mechanism. Large KVs are handled here (not too hard).
INVARIANT: a chunk is always written after the redo log.
After the chunk is written, its structures are unpinned from memory.

When a read comes in, the actual value may be in any one of the shard's chunks, so we may have to look in many of them. Disk reads are performed in the main thread.

To determine which chunks to open, the chunk tree for the shard is examined, and the top level chunks are used.

This means once two chunks are merged, their in-memory buffers are "wasted". Help this at least by keeping the Bloom filter and the index page in memory.

When a chunk is read from disk, an in-memory representation is created. These pages may be purged from memory if needed. These are put on a LRU list.

Log commit
----------

Disk IO occurs when the redo log is commited. Disk IO is issued in another thread. Hence the Commit() has an OnCommit() callback. Until the commit completes, writes are refused with an appropriate return code.

Cursors
-------

When a cursor is opened, we potentionally have to look in several places. We do that, and then merge all these pages into the cursor's own structure.

(Also support bulk copying.)

Chunk writing
-------------

When a chunk is to be written to disk, the main thread wakes the writer thread, which performs the disk write.
INVARIANT: The in-memory, pinned, completed chunk structure is read-only, as both threads may use it at this point.
When it has finished writing, it calls back to the main thread, signaling that the structure may be unpinned. The main thread then issues the next structure for writing, if any.

Chunk merging
---------------

A background thread continuously monitors chunk files and merges them. In order to avoid continually reading the directories, the rewriter thread is awaken by the chunkwriter thread when a new chunk is written. The rewriter thread reads both files from disk, and merges them on the fly, per-page, creating a new file. This creates a new file on disk, which includes both older files.

The older files may now be deleted. The rewriter thread signals the main thread that a new chunk has been created. The main thread inserts the new chunk into its tree of chunks. Eventually, once its done using it, the main thread will issue a delete file command to the chunk remover thread.

Redo log archiving
------------------

When a redo log segment file is created, a number of corresponding chunk files are created. When all these chunk files are written to disk, the segment file itself may be archived. To keep track of this, the segment structure, both in-memory and on disk, keeps track of the shardID/chunkID of chunk files that it corresponds to. Once these are all written to disk, the main thread signals the archiver thread to do whatever it wants with the segment file.

What happens if a segment file is still on disk, but its corresponding chunk files have all been rewritten. This is easily handled because if a chunk file with a higher chunkID is on disk, it means that the lower one was at one point written to disk.

INVARIANT: chunkIDs are increasing (per shardID), if n is on disk, it means that all 0..n-1 were written to disk, and their mutations are somewhere on disk.

Metadata
--------

The engine maintains a separate meta database file, which stores a list of shard(db, table) records. These are also stored in memory.

Metadata writes
---------------

When a metadata write occurs, it is written into the recovery log and commited. Then the meta database is rewritten. The meta database file contains a redo log (segmentID, commandID) pair so we know which snapshot we have. This file is written to a metadb.new file, commited, then the old file is deleted and the file is renamed to metadb. This has to be handled during opening of the db.

Shard splitting
---------------

When a shard is split, a corresponding record is written to the redo log. Then the shards in-memory chunk is split into two new chunks.

Threading model
---------------
Threads:
- main (performs reads)
- log writer (performs log writes and commits)
- chunk writer
- chunk merger
- chunk remover
- archiver

Recovery: redo log
------------------

Open the all redo log files, and replay it in-memory, creating the appropriate chunk files in-memory.
INVARIANT: if a redo log file has not been properly closed, none of its corresponding chunk files were written to disk.
BUT: if a redo log file has been properly closed, it is still possible that its corresponding chunk files were not written to disk!
QUESTION: what happens if a redo log is present, but a chunk file has already been written.

The writes in the redo log file each have a shardID associated. All the redo logs are replayed, and the meta db is also replayed. Then, all chunk files which are not used by shards are deleted by the regular garbage collection methods.

Recovery: chunk files
---------------------

Easy: Delete partially written chunk files.

CRCs
----

Add a 32-bit CRC per data page written to disk.

Delta compression
-----------------

Use 2 bytes for key size, 4 bytes for value size. Use 2 bytes for delta compression scheme.
(2 bytes delta compared to previous key) + (2 bytes key length) + (4 bytes value length) + (key) + (value)

Use 1+1 bytes per KV on disk for key delta compression.
